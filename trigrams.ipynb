{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Third-order Letter Approximation Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Loading the Text\n",
    "\n",
    "Firstly the code downloads the needed texts from the specified urls.\n",
    "\n",
    "It then creates a directory if one does not exist and downloads and decodes each book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'downloads\\The_Jungle_Book.txt' exists. Overwriting...\n",
      "File 'downloads\\A_Christmas_Carol.txt' exists. Overwriting...\n",
      "File 'downloads\\Alice_in_Wonderland.txt' exists. Overwriting...\n",
      "File 'downloads\\The_Great_Gatsby.txt' exists. Overwriting...\n",
      "File 'downloads\\Moby_Dick.txt' exists. Overwriting...\n",
      "All books have been downloaded and saved in the 'downloads' directory.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "# For the opening and fetching of URLS https://docs.python.org/3/library/urllib.request.html\n",
    "import urllib.request\n",
    "# For creating and managing directories https://docs.python.org/3/library/os.html\n",
    "import os  \n",
    "\n",
    "# Define URLs for the chosen Books, BookName:BookURL\n",
    "urls = {\n",
    "    \"The Jungle Book\": \"https://www.gutenberg.org/cache/epub/236/pg236.txt\",\n",
    "    \"A Christmas Carol\": \"https://www.gutenberg.org/cache/epub/46/pg46.txt\",\n",
    "    \"Alice in Wonderland\": \"https://www.gutenberg.org/cache/epub/11/pg11.txt\",\n",
    "    \"The Great Gatsby\": \"https://www.gutenberg.org/cache/epub/64317/pg64317.txt\",\n",
    "    \"Moby Dick\": \"https://www.gutenberg.org/cache/epub/2701/pg2701.txt\"\n",
    "}\n",
    "\n",
    "# Create a directory for downloads if it doesn't exist\n",
    "download_dir = \"downloads\"\n",
    "\n",
    "# IF the directory does not allready exist create it\n",
    "if not os.path.exists(download_dir):\n",
    "    print(f\"Directory '{download_dir}' not found. Creating it...\")\n",
    "    os.makedirs(download_dir)\n",
    "\n",
    "# Dictionary to store the content of each book\n",
    "books_content = {}\n",
    "\n",
    "# Loop through each URL and fetch the content of each page\n",
    "for book, url in urls.items():\n",
    "    # Combine the directory path with the file name after it has been correctly formatted\n",
    "    file_path = os.path.join(download_dir, f\"{book.replace(' ', '_')}.txt\")\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if os.path.exists(file_path):\n",
    "        # Overite file if it exists with the newer version\n",
    "        print(f\"File '{file_path}' exists. Overwriting...\")\n",
    "    else:\n",
    "        # Else create a new one\n",
    "        print(f\"File '{file_path}' does not exist. Downloading...\")\n",
    "    \n",
    "    # Fetch and decode the content\n",
    "    # Open Url and fetch respones\n",
    "    response = urllib.request.urlopen(url)\n",
    "    # read and decode response to readable utf-8\n",
    "    content = response.read().decode('utf-8')\n",
    "\n",
    "    # Store downloaded content into a Dictionary\n",
    "    books_content[book] = content\n",
    "\n",
    "    # Save the content to the file overwriting if it allready exists\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(content)\n",
    "\n",
    "print(f\"All books have been downloaded and saved in the '{download_dir}' directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Processing the Text\n",
    "\n",
    "With specified start and end markers that are present in all project gutenberg files the required text is extracted and processed into all caps and only using A-Z characters as well as the space and period characters.\n",
    "\n",
    "The processed texts are then saved into a processed directory as individual files wile not completely necessary it makes it easier to inspect the results individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Text for The Jungle Book...\n",
      "Processing Text for A Christmas Carol...\n",
      "Processing Text for Alice in Wonderland...\n",
      "Processing Text for The Great Gatsby...\n",
      "Processing Text for Moby Dick...\n",
      "All books have been processed and saved in the 'downloads/processed' directory.\n"
     ]
    }
   ],
   "source": [
    "# For finding and replacing unwanted characters and sections of the text https://docs.python.org/3/library/re.html\n",
    "import re\n",
    "\n",
    "def clean_text(raw_text):\n",
    "    \n",
    "    # Identify the main text content\n",
    "    # All Gutenberg EBooks have this section in their books making it easy to trim the start and end\n",
    "    start_marker = \"*** START OF THIS PROJECT GUTENBERG EBOOK\"\n",
    "    end_marker = \"*** END OF THIS PROJECT GUTENBERG EBOOK\"\n",
    "    \n",
    "    # Extract content between the markers\n",
    "    # Start Extraction from\n",
    "    start_index = raw_text.find(start_marker)\n",
    "    # End Extraction at\n",
    "    end_index = raw_text.find(end_marker)\n",
    "    \n",
    "    # If the markers exist in the text\n",
    "    if start_index != -1 and end_index != -1:\n",
    "        # Extract the text between the markers\n",
    "        text = raw_text[start_index + len(start_marker):end_index]\n",
    "    else:\n",
    "        # If the markers are not found use the entire text\n",
    "        text = raw_text \n",
    "    \n",
    "    # Remove unwanted characters and convert to uppercase\n",
    "    # Replace line breaks and tabs with spaces\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "\n",
    "    # Remove extra spaces and replace multiple spaces witha  single 1\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    # Convert all text to upper case and then remove any character that is not A-Z a \" \" or a \".\"\n",
    "    cleaned_text = re.sub(r\"[^A-Z\\s\\.]\", \"\", text.upper())\n",
    "    # Return the cleaned up text\n",
    "    return cleaned_text\n",
    "\n",
    "# Dictionary for processed books\n",
    "processed_books = {}\n",
    "\n",
    "# For each unprocessed book\n",
    "for book, raw_text in books_content.items():\n",
    "    # Log the processing og the book\n",
    "    print(f\"Processing Text for {book}...\")\n",
    "\n",
    "    # Store the processed text in the processed dictionary\n",
    "    processed_books[book] = clean_text(raw_text)\n",
    "\n",
    "# Save processed text locally in a processed folder inside the download directory\n",
    "processed_dir = os.path.join(download_dir, \"processed\")\n",
    "# Avoids errors if the directory allready exists\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "# For each processed book\n",
    "for book, content in processed_books.items():\n",
    "    # save in the correct directory with the correctly formatted filename\n",
    "    file_path = os.path.join(processed_dir, f\"{book.replace(' ', '_')}_processed.txt\")\n",
    "    # Write to the file\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(content)\n",
    "\n",
    "# Log task completion to the console\n",
    "print(\"All books have been processed and saved in the 'downloads/processed' directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Trigram Extraction\n",
    "\n",
    "For each of the processed texts a dictonary of trigrams and the number of times they actually appear in the text is created.\n",
    "\n",
    "The trigram models are also saved as their own files for each book for individual perusal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting trigrams for The Jungle Book...\n",
      "Extracting trigrams for A Christmas Carol...\n",
      "Extracting trigrams for Alice in Wonderland...\n",
      "Extracting trigrams for The Great Gatsby...\n",
      "Extracting trigrams for Moby Dick...\n",
      "Trigrams extracted and saved in the 'downloads/trigrams' directory.\n"
     ]
    }
   ],
   "source": [
    "def extract_trigrams(cleaned_text):\n",
    "    # Function to extract trigrams from the Cleaned Text and count the amount of times each occurs\n",
    "    # Dictionary for trigram and its count to be stored in\n",
    "    trigram_counts = {}\n",
    "    # Calculate the length of the text in order to calculate how many trigrams can be distracted\n",
    "    text_length = len(cleaned_text)\n",
    "    \n",
    "    # For each trigram available in the text\n",
    "    # Text length -2 as the last three characters are the last trigram that can be read\n",
    "    for i in range(text_length - 2):\n",
    "        # Extract three consecutive characters\n",
    "        trigram = cleaned_text[i:i+3]\n",
    "        # If the trigram is allready extracted\n",
    "        if trigram in trigram_counts:\n",
    "            # Add to the trigrams count in the dictonary\n",
    "            trigram_counts[trigram] += 1\n",
    "        else:\n",
    "            # Else create a new entry and set its count to 1\n",
    "            trigram_counts[trigram] = 1\n",
    "    \n",
    "    # Return the filled dictonary\n",
    "    return trigram_counts\n",
    "\n",
    "# Generate Trigram counts for each book and store\n",
    "trigram_models = {}\n",
    "\n",
    "# For each processed book\n",
    "for book, cleaned_text in processed_books.items():\n",
    "    # Console log the book being extracted from\n",
    "    print(f\"Extracting trigrams for {book}...\")\n",
    "    # Store the trigram model with the books title as the key\n",
    "    trigram_models[book] = extract_trigrams(cleaned_text)\n",
    "\n",
    "# Save extracted trigram  models locally, so each can be perused in the case of errors\n",
    "trigram_dir = os.path.join(download_dir, \"trigrams\")\n",
    "os.makedirs(trigram_dir, exist_ok=True)\n",
    "\n",
    "for book, trigram_counts in trigram_models.items():\n",
    "    # Save in the correct directory with the correctly formatted filename\n",
    "    file_path = os.path.join(trigram_dir, f\"{book.replace(' ', '_')}_trigrams.txt\")\n",
    "    # Writes each count to the file in a readable format\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        for trigram, count in trigram_counts.items():\n",
    "            file.write(f\"{trigram}: {count}\\n\")\n",
    "\n",
    "#  Log to console once the process is complete\n",
    "print(\"Trigrams extracted and saved in the 'downloads/trigrams' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Single Text Test\n",
    "\n",
    "In this code a single books trigrams model is tested to see what reults are returned from the extracted trigrams, as well as seeing the most common ones ordered by count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 trigrams for The Jungle Book:\n",
      " TH: 6341\n",
      "THE: 5272\n",
      "HE : 4776\n",
      "ND : 2735\n",
      " AN: 2633\n",
      "AND: 2548\n",
      "ED : 1764\n",
      " TO: 1629\n",
      " HE: 1611\n",
      "NG : 1581\n",
      "\n",
      "Summary for The Jungle Book:\n",
      "Total trigrams: 279916\n",
      "Unique trigrams: 4541\n"
     ]
    }
   ],
   "source": [
    "# Select a single book to test (e.g., The Jungle Book)\n",
    "test_book = \"The Jungle Book\"\n",
    "\n",
    "# Retrieve the trigram model for the test book\n",
    "test_trigrams = trigram_models[test_book]\n",
    "\n",
    "# Sort trigrams by frequency for better readability\n",
    "sorted_trigrams = sorted(\n",
    "    # Convert the dictonary into a list of tuples trigram,count\n",
    "    test_trigrams.items(),\n",
    "    # sort by the second elemnet of the tuple which is the count\n",
    "    key=lambda x: x[1], \n",
    "    # Have them sorted in descending order\n",
    "    reverse=True)\n",
    "\n",
    "# Display the top 10 most frequent trigrams\n",
    "print(f\"Top 10 trigrams for {test_book}:\")\n",
    "for trigram, count in sorted_trigrams[:10]:\n",
    "    print(f\"{trigram}: {count}\")\n",
    "\n",
    "# Calculate trigram totals\n",
    "total_trigrams = sum(test_trigrams.values())\n",
    "unique_trigrams = len(test_trigrams)\n",
    "\n",
    "# Log results to console\n",
    "print(f\"\\nSummary for {test_book}:\")\n",
    "print(f\"Total trigrams: {total_trigrams}\")\n",
    "print(f\"Unique trigrams: {unique_trigrams}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Combining All Texts\n",
    "\n",
    "In this code snippet the processed text for each book is combined in a single text for the sake of easier processing of a trigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining all processed texts...\n",
      "Extracting trigrams from the combined text...\n",
      "Combined trigram data saved to 'downloads\\trigrams\\combined_trigrams.txt'.\n",
      "\n",
      "Summary of the combined trigram model:\n",
      "Total trigrams: 2075008\n",
      "Unique trigrams: 7181\n"
     ]
    }
   ],
   "source": [
    "# Combine all processed texts into a single string\n",
    "print(\"Combining all processed texts...\")\n",
    "# Join all books text with a space in between each entry\n",
    "combined_text = \" \".join(processed_books.values())\n",
    "\n",
    "# Generate the combined trigram model\n",
    "print(\"Extracting trigrams from the combined text...\")\n",
    "# Call the previous code to extract a trigram model from the combined text\n",
    "combined_trigrams = extract_trigrams(combined_text)\n",
    "\n",
    "# Save the combined trigram model for comparision for with the individual files\n",
    "combined_file_path = os.path.join(trigram_dir, \"combined_trigrams.txt\")\n",
    "# Write the data in a readable format and have it ordered by total count in descending order\n",
    "with open(combined_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    for trigram, count in sorted(combined_trigrams.items(), key=lambda x: x[1], reverse=True):\n",
    "        file.write(f\"{trigram}: {count}\\n\")\n",
    "# Log that the file aas been saved\n",
    "print(f\"Combined trigram data saved to '{combined_file_path}'.\")\n",
    "\n",
    "# The same way we did with the single text calculate the trigram model totals\n",
    "total_trigrams = sum(combined_trigrams.values())\n",
    "unique_trigrams = len(combined_trigrams)\n",
    "\n",
    "\n",
    "# Log results\n",
    "print(\"\\nSummary of the combined trigram model:\")\n",
    "print(f\"Total trigrams: {total_trigrams}\")\n",
    "print(f\"Unique trigrams: {unique_trigrams}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Testing Results\n",
    "In the final code of the task the results of the comined texts extracted trigram model is explored and displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing trigram extraction for the combined text...\n",
      "Top 10 trigrams in the combined text:\n",
      " TH: 43561\n",
      "THE: 34210\n",
      "HE : 30009\n",
      "ND : 15678\n",
      " AN: 15222\n",
      "ED : 14840\n",
      "AND: 14760\n",
      "ING: 13369\n",
      "NG : 12849\n",
      " OF: 11448\n",
      "\n",
      "Summary for the combined text:\n",
      "Total trigrams: 2075008\n",
      "Unique trigrams: 7181\n"
     ]
    }
   ],
   "source": [
    "# Log Start of process to console\n",
    "print(\"Testing trigram extraction for the combined text...\")\n",
    "# Sort trigrams by frequency for the combined text\n",
    "sorted_combined_trigrams = sorted(combined_trigrams.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display the top 10 most frequent trigrams\n",
    "print(\"Top 10 trigrams in the combined text:\")\n",
    "for trigram, count in sorted_combined_trigrams[:10]:\n",
    "    print(f\"{trigram}: {count}\")\n",
    "\n",
    "# Calculate totals\n",
    "total_trigrams = sum(combined_trigrams.values())\n",
    "unique_trigrams = len(combined_trigrams)\n",
    "\n",
    "# Log Totals to console for a final time\n",
    "print(\"\\nSummary for the combined text:\")\n",
    "print(f\"Total trigrams: {total_trigrams}\")\n",
    "print(f\"Unique trigrams: {unique_trigrams}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Third-Order Letter Approximation Generation\n",
    "\n",
    "In this task, the trigram model from Task 1 is used to generate a string of 10,000 characters. The generated text is created by predicting each subsequent character based on the previous two characters, leveraging the probabilities stored in the trigram model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the generation of random numbers\n",
    "import random\n",
    "\n",
    "def generate_text(trigram_model, start_text, length):\n",
    "    # Function to generate a text of set length based on previoulsy generated trigram model\n",
    "\n",
    "    # Add the start text to the begining of the generated string\n",
    "    generated_text = start_text\n",
    "\n",
    "    # Build a trigram lookup for easier access using this dictionary\n",
    "    trigram_lookup = {}\n",
    "\n",
    "    # For each trigram and its count return as a tuple\n",
    "    for trigram, count in trigram_model.items():\n",
    "        # Extract first two characters as the prefix\n",
    "        prefix = trigram[:2]\n",
    "        # If the prefix has not been added to the triagram lookup i is added with the prefix as key\n",
    "        if prefix not in trigram_lookup:\n",
    "            trigram_lookup[prefix] = []\n",
    "        # Adds a tuple of next_char, count to the prefix\n",
    "        trigram_lookup[prefix].append((trigram[2], count))\n",
    "\n",
    "    # Generate text by iterating until the desired length is reached\n",
    "    while len(generated_text) < length:\n",
    "        # Get the last two characters\n",
    "        prefix = generated_text[-2:]\n",
    "\n",
    "        # If it is in the trigram lookup\n",
    "        if prefix in trigram_lookup:\n",
    "        # Get possible continuations and their counts\n",
    "            continuations = trigram_lookup[prefix]\n",
    "            # Separate characters and weights\n",
    "            characters, weights = zip(*continuations)\n",
    "        else:\n",
    "            # If it is not in the lookup, randomly select a trigram from the entire model\n",
    "            all_trigrams = list(trigram_model.items())\n",
    "            # Choose a random trigram as fallback\n",
    "            fallback_trigram, fallback_count = random.choice(all_trigrams)\n",
    "            # Use the third character of the trigram\n",
    "            characters, weights = [fallback_trigram[2]], [fallback_count]\n",
    "\n",
    "        # Randomly select the next character using weights and returns the character\n",
    "        next_char = random.choices(characters, weights=weights)[0]\n",
    "        # Append the character to the generated text\n",
    "        generated_text += next_char\n",
    "\n",
    "    # Return text\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: String Generation and Saving\n",
    "Using the method coded in the previous snippet a string of 10,000 characters is generted from the starting point of \"TH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text...\n",
      "Generated text saved to 'downloads\\trigrams\\generated_text.txt'.\n"
     ]
    }
   ],
   "source": [
    "# Declare the starting string\n",
    "start_text = \"TH\"\n",
    "# String length\n",
    "length = 10000\n",
    "# Log start of process to console\n",
    "print(\"Generating text...\")\n",
    "# Call generation method\n",
    "generated_text = generate_text(combined_trigrams, start_text, length)\n",
    "\n",
    "# Save the generated text for inspection\n",
    "generated_text_file = os.path.join(trigram_dir, \"generated_text.txt\")\n",
    "with open(generated_text_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(generated_text)\n",
    "\n",
    "#Log File generation to console\n",
    "print(f\"Generated text saved to '{generated_text_file}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Analyze the Model\n",
    "\n",
    "Using the text generated in the previous task, and the file of valid english words contained in words.txt the amount of correct words generated will be counted and analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Words\n",
    "\n",
    "The list of valid words must first be loaded into the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_list(file_path):\n",
    "    # Code to load the valid words into python for use\n",
    "    with open(file_path, \"r\") as file:\n",
    "        # Blank Space is stripped and all chaarcters are converted to upper case\n",
    "        # Formatted words are stored in a set\n",
    "        word_set = {line.strip().upper() for line in file}\n",
    "    # Log to console\n",
    "    print(f\"Word Set Loaded.\")\n",
    "    # Return the extracted set \n",
    "    return word_set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
